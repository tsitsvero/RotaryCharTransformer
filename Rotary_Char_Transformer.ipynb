{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTHGYS3cNBkU",
        "outputId": "8b509be6-0d2b-4df9-e22f-3a8ce17df275"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clone the NanoGPT repository and install necessary dependencies\n",
        "\n",
        "# Clone the NanoGPT repository\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "%cd nanoGPT\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets tiktoken wandb tqdm numpy torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "38324ba39cc04486aed54fae9143af15",
            "77e4540a661442248cdd4067fecf4eca",
            "f11665315e80406e83b03afb08c69ccd",
            "591db38f2adc44a9821e231761c66ae5",
            "8286acc6f17343f19f494768e69b4977",
            "9281f2f5f2e84d688ec511bb6d05e0b3",
            "f94bc6c7260c448caceb6f5921e518a5",
            "ef5bc403174543aa86db68ec5624e80d",
            "dcba5356ffcf4d3aaad345426363d918",
            "5c1c3067d3a240f3bec061d7dfa24201",
            "45df0e8172b840f2ba63338246d17464",
            "d1b47e9d43e44e899757587476831fad",
            "4eac29f722ce447fbd831b17b5ceff16",
            "cb2a935ac9fb495f9b74c3e212b25078",
            "552d0d9929df484cbffb2f3e5c5269ef",
            "3e2f2c46720141c5bc55f5a800661767",
            "fff687de55884d25aecebfca802e3112",
            "dce6fe3abe5f4b789d7bd2e65b778f11",
            "a3e50a7c49f44f7ab78a8dd53b4dce13",
            "87716ff4d7e44c799e93b6571a61cd08",
            "60e81221f54d4b46bf51f7db3a2ccffc",
            "53ff6026f9814daf8d24aae2c5e879ae",
            "ed1f2ea6ab70438e959f01067593456b",
            "86d6c0a80f8548e3a93bd96ea04477ed",
            "2c4815ba89064fddbe809d4a108eae39",
            "52844cdd0e184b349abcc529655f3af7",
            "76a970bc4d0849b09365530530e00a5a",
            "e687cf062bd542098b32f821d49d4fc8",
            "bcd8de3f4b164025bed6cbafc159b0e2",
            "93d88e7e93c04af5b55990c7dc2b9d32",
            "bc9df5b5a1cd4a208c7ca08005780269",
            "880ed764e5d94768a1f99506b1bbd4b7",
            "a930af4ad92940bf87877f789362fc3f",
            "118b95bec92c4c1f9c14def5c8f60d4f",
            "57544fb85c434a80b9826bd0bc1836f8",
            "c0c4824ee555440894ced5372b729712",
            "570f62562a35410ba0e7208f6e6e1ff0",
            "00704724d5ff48898a7452ad4e54eb55",
            "7c30a1b894134bebb16689211b7f6c43",
            "140d48fd8b5c49939a395b47034ab2e9",
            "6f1bada96d4d41debc8f9970b8557e77",
            "d8280cddc3f042298f8d860518fce268",
            "0d5ca73b84a54b6fa7e7878374871fbd",
            "f07f0feac44d4e17bf7a1c555338e280"
          ]
        },
        "id": "vJOPMyhtNHJf",
        "outputId": "4264cd2e-993d-4c80-e037-e27f9966d112"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38324ba39cc04486aed54fae9143af15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "enwik8.py:   0%|          | 0.00/2.94k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1b47e9d43e44e899757587476831fad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/4.28k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for LTCB/enwik8 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/LTCB/enwik8.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed1f2ea6ab70438e959f01067593456b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/36.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "118b95bec92c4c1f9c14def5c8f60d4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1128024 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of characters in the dataset: 97492430\n",
            "Data successfully split and saved as train.txt, valid.txt, and test.txt\n",
            "Training characters: 90000000\n",
            "Validation characters: 5000000\n",
            "Test characters: 2492430\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the enwik8 dataset\n",
        "dataset = load_dataset(\"LTCB/enwik8\", split=\"train\")\n",
        "\n",
        "# Extract the raw text from the dataset\n",
        "raw_text = ''.join(dataset['text'])\n",
        "\n",
        "# Calculate the total number of characters in the dataset\n",
        "total_characters = len(raw_text)\n",
        "print(f\"Total number of characters in the dataset: {total_characters}\")\n",
        "\n",
        "# Define the number of characters for training\n",
        "train_character_limit = 90_000_000\n",
        "\n",
        "# Ensure sufficient data is available for training\n",
        "if total_characters < train_character_limit:\n",
        "    raise ValueError(f\"Insufficient data for training. Only {total_characters} characters available.\")\n",
        "\n",
        "# Determine the remaining characters for validation and testing\n",
        "remaining_characters = total_characters - train_character_limit\n",
        "\n",
        "# Define validation character size (maximum of 5 million)\n",
        "validation_size = min(5_000_000, remaining_characters)\n",
        "\n",
        "# Use the rest for testing\n",
        "test_size = remaining_characters - validation_size\n",
        "\n",
        "# Split the text into training, validation, and test sets\n",
        "train_data = raw_text[:train_character_limit]\n",
        "validation_data = raw_text[train_character_limit:train_character_limit + validation_size]\n",
        "test_data = raw_text[train_character_limit + validation_size:]\n",
        "\n",
        "# Save each split to a separate file\n",
        "with open('train.txt', 'w') as train_file:\n",
        "    train_file.write(train_data)\n",
        "with open('valid.txt', 'w') as valid_file:\n",
        "    valid_file.write(validation_data)\n",
        "with open('test.txt', 'w') as test_file:\n",
        "    test_file.write(test_data)\n",
        "\n",
        "print(\"Data successfully split and saved as train.txt, valid.txt, and test.txt\")\n",
        "\n",
        "# Verify the sizes of the splits\n",
        "print(f\"Training characters: {len(train_data)}\")\n",
        "print(f\"Validation characters: {len(validation_data)}\")\n",
        "print(f\"Test characters: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vRjXyu9gNKl3"
      },
      "outputs": [],
      "source": [
        "# Step 3: Prepare the Data for NanoGPT\n",
        "\n",
        "# Create a directory for the dataset\n",
        "!mkdir -p data/enwik8\n",
        "\n",
        "# Move the data files into the dataset directory\n",
        "!mv train.txt valid.txt test.txt data/enwik8/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuSfloAhNOjP",
        "outputId": "9de856f3-ae3b-4cc6-f176-fe47ec56f8f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing data/prepare_enwik8.py\n"
          ]
        }
      ],
      "source": [
        "# Create a new script called prepare_enwik8.py\n",
        "\n",
        "%%writefile data/prepare_enwik8.py\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Define the data directory where the files will be stored\n",
        "data_dir = 'data/enwik8'\n",
        "\n",
        "# Safely read the text files, ensuring they exist\n",
        "def read_file_safe(filename):\n",
        "    filepath = os.path.join(data_dir, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"File {filename} not found in {data_dir}\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "# Load the dataset splits\n",
        "train_data = read_file_safe('train.txt')\n",
        "val_data = read_file_safe('valid.txt')\n",
        "test_data = read_file_safe('test.txt')\n",
        "\n",
        "# Extract unique characters from the training data\n",
        "chars = sorted(set(train_data))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary size (unique characters): {vocab_size}\")\n",
        "\n",
        "# Create character-to-integer (stoi) and integer-to-character (itos) mappings\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Save the mappings as a metadata file for future use\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "meta_filepath = os.path.join(data_dir, 'meta.pkl')\n",
        "with open(meta_filepath, 'wb') as f:\n",
        "    pickle.dump(meta, f)\n",
        "print(f\"Saved metadata to {meta_filepath}\")\n",
        "\n",
        "# Function to encode data into integer token IDs\n",
        "def encode_data(text):\n",
        "    return [stoi[ch] for ch in text if ch in stoi]\n",
        "\n",
        "# Encode the training, validation, and test data\n",
        "train_ids = np.array(encode_data(train_data), dtype=np.uint16)\n",
        "val_ids = np.array(encode_data(val_data), dtype=np.uint16)\n",
        "test_ids = np.array(encode_data(test_data), dtype=np.uint16)\n",
        "\n",
        "# Save the encoded data to binary files for efficient loading during training\n",
        "train_bin_filepath = os.path.join(data_dir, 'train.bin')\n",
        "val_bin_filepath = os.path.join(data_dir, 'val.bin')\n",
        "test_bin_filepath = os.path.join(data_dir, 'test.bin')\n",
        "\n",
        "train_ids.tofile(train_bin_filepath)\n",
        "val_ids.tofile(val_bin_filepath)\n",
        "test_ids.tofile(test_bin_filepath)\n",
        "\n",
        "print(f\"Data preparation complete. Files saved at: \\n- {train_bin_filepath}\\n- {val_bin_filepath}\\n- {test_bin_filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af-vxSvMNQ5h",
        "outputId": "663ff41a-93a1-4d38-fef8-ac158d13e2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size (unique characters): 5486\n",
            "Saved metadata to data/enwik8/meta.pkl\n",
            "Data preparation complete. Files saved at: \n",
            "- data/enwik8/train.bin\n",
            "- data/enwik8/val.bin\n",
            "- data/enwik8/test.bin\n"
          ]
        }
      ],
      "source": [
        "!python data/prepare_enwik8.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstHZcCcNRcX",
        "outputId": "6bb90ea8-8b7a-4d55-f9fd-dec3cb34e04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config/enwik8_char_rope.py\n"
          ]
        }
      ],
      "source": [
        "# Create a new configuration file for the modified model\n",
        "\n",
        "%%writefile config/enwik8_char_rope.py\n",
        "import math\n",
        "\n",
        "# Configuration for the modified model\n",
        "out_dir = 'out-enwik8-char-rope'  # Output directory for model checkpoints and logs\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "log_interval = 100\n",
        "\n",
        "always_save_checkpoint = True  # Ensure we save checkpoints\n",
        "wandb_log = False\n",
        "wandb_project = 'enwik8-char'\n",
        "wandb_run_name = 'gpt2-enwik8-char-rope'\n",
        "\n",
        "dataset = 'enwik8'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64  # Adjust based on your GPU memory\n",
        "block_size = 256  # Context length\n",
        "\n",
        "# Model parameters\n",
        "n_layer = 8\n",
        "n_head = 8\n",
        "n_embd = 512\n",
        "dropout = 0.1  # Added some dropout for regularization\n",
        "bias = False  # No bias in LayerNorm and Linear layers\n",
        "\n",
        "# Optimization parameters\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5000  # Number of iterations for training\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-4\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "weight_decay = 0.1\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 100\n",
        "init_from = 'scratch'  # Initialize model from scratch\n",
        "\n",
        "# Use the modified model\n",
        "model_type = 'rope'\n",
        "\n",
        "# System parameters\n",
        "device = 'cuda'  # Use CUDA for training\n",
        "dtype = 'float16'  # Use float16 for faster training\n",
        "compile = False  # Disable compilation for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EePlEIIcNfLg",
        "outputId": "f011a97e-554d-4ee5-944e-f0762b988b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing model_rope.py\n"
          ]
        }
      ],
      "source": [
        "# Write the modified model with RoPE\n",
        "\n",
        "%%writefile model_rope.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from model import GPTConfig\n",
        "import inspect\n",
        "\n",
        "def apply_rotary_pos_emb(q, cos, sin):\n",
        "    # Apply rotary position embedding to query and key\n",
        "    q_cos = q * cos\n",
        "    q_sin = q * sin\n",
        "    q_rotated = q_cos + rotate_half(q_sin)\n",
        "    return q_rotated\n",
        "\n",
        "\n",
        "def rotate_half(x):\n",
        "    # Helper function to apply rotation\n",
        "    x1 = x[..., :x.shape[-1]//2]\n",
        "    x2 = x[..., x.shape[-1]//2:]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "class GPTWithRoPE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # Report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wte.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        # Token embeddings\n",
        "        tok_emb = self.transformer.wte(idx)  # shape (b, t, n_embd)\n",
        "\n",
        "        x = self.transformer.drop(tok_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # Start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # Filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # Create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "        # Precompute rotary embeddings\n",
        "        self.rotary_emb = RotaryEmbedding(dim=config.n_embd // config.n_head)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x).view(B, T, 3, self.n_head, C // self.n_head).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, n_head, T, head_dim)\n",
        "\n",
        "        # Apply rotary embeddings to q and k\n",
        "        q, k = self.rotary_emb(q, k)  # Correcting this line\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, q, k):\n",
        "        t = q.size(-2)\n",
        "        freqs = torch.einsum(\"i,j->ij\", torch.arange(t, device=q.device).float(), self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        cos = emb.cos()[None, None, :, :]\n",
        "        sin = emb.sin()[None, None, :, :]\n",
        "        q = apply_rotary_pos_emb(q, cos, sin)\n",
        "        k = apply_rotary_pos_emb(k, cos, sin)  # Fix the call for 'k'\n",
        "        return q, k\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = F.gelu(x)  # Use standard GELU\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0EX1o4XOarw",
        "outputId": "15fc4d1a-19ef-4241-851d-858813e6cfe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from tqdm import tqdm\n",
        "\n",
        "from model import GPTConfig\n",
        "from model_baseline import BaselineGPT\n",
        "from model_rope import GPTWithRoPE\n",
        "\n",
        "def get_serializable_config(config):\n",
        "    return {k: v for k, v in config.items() if isinstance(v, (int, float, str, bool, type(None))) and not k.startswith('__')}\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, required=True, help='Configuration file')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config_file = args.config\n",
        "    config = {}\n",
        "    with open(config_file, 'r') as f:\n",
        "        exec(f.read(), {}, config)\n",
        "\n",
        "    config = {k: v for k, v in config.items() if not k.startswith('__')}\n",
        "\n",
        "    if 'out_dir' not in config:\n",
        "        print(\"Error: 'out_dir' not specified in the configuration file.\")\n",
        "        return\n",
        "\n",
        "    if int(os.environ.get('RANK', -1)) == -1:\n",
        "        os.makedirs(config['out_dir'], exist_ok=True)\n",
        "        print(f\"Output directory: {config['out_dir']}\")\n",
        "\n",
        "    ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "    if ddp:\n",
        "        init_process_group(backend='nccl')\n",
        "        ddp_rank = int(os.environ['RANK'])\n",
        "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "        device = f'cuda:{ddp_local_rank}'\n",
        "        torch.cuda.set_device(device)\n",
        "        master_process = ddp_rank == 0\n",
        "        config['gradient_accumulation_steps'] //= ddp_world_size\n",
        "    else:\n",
        "        master_process = True\n",
        "        ddp_world_size = 1\n",
        "        device = config['device']\n",
        "\n",
        "    tokens_per_iter = (config['gradient_accumulation_steps'] * ddp_world_size *\n",
        "                       config['batch_size'] * config['block_size'])\n",
        "    print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "    torch.manual_seed(1337 + int(time.time()))\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[config['dtype']]\n",
        "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "    data_dir = os.path.join('data', config['dataset'])\n",
        "\n",
        "    def get_batch(split):\n",
        "        data_path = os.path.join(data_dir, f'{split}.bin')\n",
        "        data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
        "        ix = torch.randint(len(data) - config['block_size'], (config['batch_size'],))\n",
        "        x = torch.stack([torch.from_numpy((data[i:i+config['block_size']]).astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy((data[i+1:i+1+config['block_size']]).astype(np.int64)) for i in ix])\n",
        "        if device_type == 'cuda':\n",
        "            x = x.pin_memory().to(device, non_blocking=True)\n",
        "            y = y.pin_memory().to(device, non_blocking=True)\n",
        "        else:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "        return x, y\n",
        "\n",
        "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    vocab_size = meta['vocab_size']\n",
        "    config['vocab_size'] = vocab_size\n",
        "\n",
        "    gpt_config_keys = ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size', 'dropout']\n",
        "    gpt_config = {k: v for k, v in config.items() if k in gpt_config_keys}\n",
        "    gptconf = GPTConfig(**gpt_config)\n",
        "\n",
        "    if config.get('model_type') == 'rope':\n",
        "        model = GPTWithRoPE(gptconf)\n",
        "        print(\"Using GPTWithRoPE model.\")\n",
        "    else:\n",
        "        model = BaselineGPT(gptconf)\n",
        "        print(\"Using BaselineGPT model.\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize optimizer outside of the model\n",
        "    decay_params = [p for p in model.parameters() if p.dim() >= 2]\n",
        "    no_decay_params = [p for p in model.parameters() if p.dim() < 2]\n",
        "\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': decay_params, 'weight_decay': config['weight_decay']},\n",
        "        {'params': no_decay_params, 'weight_decay': 0.0}\n",
        "    ], lr=config['learning_rate'], betas=(config['beta1'], config['beta2']))\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(config['dtype'] == 'float16'))\n",
        "\n",
        "    iter_num = 0\n",
        "    best_val_loss = 1e9\n",
        "\n",
        "    if config.get('init_from', 'scratch') == 'resume':\n",
        "        print(f\"Resuming training from {config['out_dir']}\")\n",
        "        ckpt_path = os.path.join(config['out_dir'], 'ckpt.pt')\n",
        "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model'], strict=False)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        iter_num = checkpoint['iter_num']\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        print(f\"Resumed from iteration {iter_num}, best val loss {best_val_loss}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Number of parameters: {total_params/1e6:.2f}M\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def estimate_loss():\n",
        "        out = {}\n",
        "        model.eval()\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(config['eval_iters'])\n",
        "            for k in range(config['eval_iters']):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "        model.train()\n",
        "        return out\n",
        "\n",
        "    def get_lr(it):\n",
        "        if it < config['warmup_iters']:\n",
        "            return config['learning_rate'] * it / config['warmup_iters']\n",
        "        if it > config['lr_decay_iters']:\n",
        "            return config['min_lr']\n",
        "        decay_ratio = (it - config['warmup_iters']) / (config['lr_decay_iters'] - config['warmup_iters'])\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "        return config['min_lr'] + coeff * (config['learning_rate'] - config['min_lr'])\n",
        "\n",
        "    X, Y = get_batch('train')\n",
        "    running_mfu = -1.0\n",
        "    t0 = time.time()\n",
        "\n",
        "    local_iter_num = 0\n",
        "    raw_model = model.module if ddp else model\n",
        "\n",
        "    with tqdm(total=config['max_iters'], desc=\"Training Progress\") as pbar:\n",
        "        while iter_num < config['max_iters']:\n",
        "            lr = config['learning_rate'] if not config['decay_lr'] else get_lr(iter_num)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "            for micro_step in range(config['gradient_accumulation_steps']):\n",
        "                if ddp:\n",
        "                    model.require_backward_grad_sync = (micro_step == config['gradient_accumulation_steps'] - 1)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                    loss = loss / config['gradient_accumulation_steps']\n",
        "                X, Y = get_batch('train')\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "            if config['grad_clip'] != 0.0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            t1 = time.time()\n",
        "            dt = t1 - t0\n",
        "            t0 = t1\n",
        "\n",
        "            if iter_num % config['eval_interval'] == 0 and master_process:\n",
        "                losses = estimate_loss()\n",
        "                print(f\"\\nStep {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "                if losses['val'] < best_val_loss or config['always_save_checkpoint']:\n",
        "                    best_val_loss = losses['val']\n",
        "                    checkpoint = {\n",
        "                        'model': raw_model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'iter_num': iter_num,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'config': get_serializable_config(config),\n",
        "                    }\n",
        "                    checkpoint_path = os.path.join(config['out_dir'], 'ckpt.pt')\n",
        "                    torch.save(checkpoint, checkpoint_path)\n",
        "                    print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "            iter_num += 1\n",
        "            local_iter_num += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            if iter_num % config['log_interval'] == 0 and master_process:\n",
        "              lossf = loss.item() * config['gradient_accumulation_steps']\n",
        "              print(f\"Iter {iter_num}: loss {lossf:.4f}\")\n",
        "\n",
        "\n",
        "    if ddp:\n",
        "        destroy_process_group()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7u7w8VMTwqr",
        "outputId": "3e9ac121-de86-453b-a492-1f7f3581d951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing model_baseline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model_baseline.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "# Define GPTBlock with MultiheadAttention\n",
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = nn.MultiheadAttention(config.n_embd, config.n_head, dropout=config.dropout)\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # Feed-forward layers\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply layer normalization\n",
        "        x_ln = self.ln_1(x)\n",
        "\n",
        "        # Self-attention uses x_ln as query, key, and value\n",
        "        attn_output, _ = self.attn(x_ln, x_ln, x_ln)\n",
        "        x = x + self.drop(attn_output)\n",
        "\n",
        "        # Feedforward block with residual connection\n",
        "        x = x + self.drop(self.mlp(self.ln_2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define the main BaselineGPT model\n",
        "class BaselineGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        # Transformer components\n",
        "        self.transformer = nn.ModuleDict({\n",
        "            'wte': nn.Embedding(config.vocab_size, config.n_embd),    # Token embedding\n",
        "            'wpe': nn.Embedding(config.block_size, config.n_embd),    # Positional embedding\n",
        "            'drop': nn.Dropout(config.dropout),                       # Dropout\n",
        "            'h': nn.ModuleList([GPTBlock(config) for _ in range(config.n_layer)]),  # Stack of GPT blocks\n",
        "            'ln_f': nn.LayerNorm(config.n_embd),                      # Final layer normalization\n",
        "        })\n",
        "\n",
        "        # Language modeling head\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize the weights for all components\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "\n",
        "        # Generate position indices and compute token and positional embeddings\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # (1, t)\n",
        "        tok_emb = self.transformer['wte'](idx)  # (b, t, n_embd)\n",
        "        pos_emb = self.transformer['wpe'](pos)  # (1, t, n_embd)\n",
        "\n",
        "        # Combine token and positional embeddings, then apply dropout\n",
        "        x = self.transformer['drop'](tok_emb + pos_emb)\n",
        "\n",
        "        # Pass through the stack of GPT blocks\n",
        "        for block in self.transformer['h']:\n",
        "            x = block(x)\n",
        "\n",
        "        # Final layer normalization\n",
        "        x = self.transformer['ln_f'](x)\n",
        "\n",
        "        # Compute logits for language modeling\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # If targets are provided, compute loss\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return logits, loss\n",
        "\n",
        "        # Return logits if no targets are provided\n",
        "        return logits, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHLerWNQOzXl",
        "outputId": "9371651f-a859-4215-8a76-6cf789086353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting config/enwik8_char_baseline.py\n"
          ]
        }
      ],
      "source": [
        "# Create a configuration file for the baseline model\n",
        "\n",
        "%%writefile config/enwik8_char_baseline.py\n",
        "out_dir = 'out-enwik8-char'\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "log_interval = 100\n",
        "\n",
        "always_save_checkpoint = True\n",
        "wandb_log = False\n",
        "wandb_project = 'enwik8-char'\n",
        "wandb_run_name = 'gpt2-enwik8-char-baseline'\n",
        "\n",
        "dataset = 'enwik8'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "\n",
        "n_layer = 12\n",
        "n_head = 8\n",
        "n_embd = 384\n",
        "dropout = 0.1\n",
        "bias = False\n",
        "\n",
        "learning_rate = 1e-4\n",
        "max_iters = 5000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "weight_decay = 0.1\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 100\n",
        "init_from = 'scratch'\n",
        "\n",
        "device = 'cuda'\n",
        "dtype = 'float16'\n",
        "compile = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3rZr5LYO18p",
        "outputId": "fdc1a4da-7e34-40c7-df75-3fa1826d31e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: out-enwik8-char\n",
            "Tokens per iteration will be: 16,384\n",
            "Using BaselineGPT model.\n",
            "/content/nanoGPT/train.py:111: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(config['dtype'] == 'float16'))\n",
            "Number of parameters: 25.61M\n",
            "Training Progress:   0% 0/5000 [00:00<?, ?it/s]\n",
            "Step 0: train loss 8.7378, val loss 8.7341\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:   2% 100/5000 [01:18<26:29,  3.08it/s]Iter 100: loss 4.2830\n",
            "Training Progress:   4% 200/5000 [01:50<25:31,  3.13it/s]Iter 200: loss 2.9377\n",
            "Training Progress:   6% 300/5000 [02:22<25:30,  3.07it/s]Iter 300: loss 2.8258\n",
            "Training Progress:   8% 400/5000 [02:55<24:49,  3.09it/s]Iter 400: loss 2.8007\n",
            "Training Progress:  10% 500/5000 [03:27<24:19,  3.08it/s]Iter 500: loss 2.7109\n",
            "\n",
            "Step 500: train loss 2.7367, val loss 2.7679\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  12% 600/5000 [04:46<24:12,  3.03it/s]Iter 600: loss 2.6838\n",
            "Training Progress:  14% 700/5000 [05:19<22:59,  3.12it/s]Iter 700: loss 2.7701\n",
            "Training Progress:  16% 800/5000 [05:51<22:40,  3.09it/s]Iter 800: loss 2.6653\n",
            "Training Progress:  18% 900/5000 [06:23<22:06,  3.09it/s]Iter 900: loss 2.6954\n",
            "Training Progress:  20% 1000/5000 [06:56<21:31,  3.10it/s]Iter 1000: loss 2.6687\n",
            "\n",
            "Step 1000: train loss 2.7191, val loss 2.7459\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  22% 1100/5000 [08:15<21:38,  3.00it/s]Iter 1100: loss 2.7728\n",
            "Training Progress:  24% 1200/5000 [08:47<20:11,  3.14it/s]Iter 1200: loss 2.6727\n",
            "Training Progress:  26% 1300/5000 [09:20<19:55,  3.09it/s]Iter 1300: loss 2.7460\n",
            "Training Progress:  28% 1400/5000 [09:52<19:26,  3.09it/s]Iter 1400: loss 2.7057\n",
            "Training Progress:  30% 1500/5000 [10:24<18:46,  3.11it/s]Iter 1500: loss 2.6572\n",
            "\n",
            "Step 1500: train loss 2.7145, val loss 2.7352\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  32% 1600/5000 [11:42<18:57,  2.99it/s]Iter 1600: loss 2.6628\n",
            "Training Progress:  34% 1700/5000 [12:15<17:33,  3.13it/s]Iter 1700: loss 2.6908\n",
            "Training Progress:  36% 1800/5000 [12:47<17:18,  3.08it/s]Iter 1800: loss 2.6904\n",
            "Training Progress:  38% 1900/5000 [13:19<16:42,  3.09it/s]Iter 1900: loss 2.6840\n",
            "Training Progress:  40% 2000/5000 [13:52<16:04,  3.11it/s]Iter 2000: loss 2.7061\n",
            "\n",
            "Step 2000: train loss 2.7084, val loss 2.7386\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  42% 2100/5000 [15:11<16:04,  3.01it/s]Iter 2100: loss 2.7033\n",
            "Training Progress:  44% 2200/5000 [15:43<15:01,  3.11it/s]Iter 2200: loss 2.7043\n",
            "Training Progress:  46% 2300/5000 [16:16<14:32,  3.09it/s]Iter 2300: loss 2.6746\n",
            "Training Progress:  48% 2400/5000 [16:48<14:05,  3.08it/s]Iter 2400: loss 2.6542\n",
            "Training Progress:  50% 2500/5000 [17:20<13:29,  3.09it/s]Iter 2500: loss 2.7310\n",
            "\n",
            "Step 2500: train loss 2.7029, val loss 2.7288\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  52% 2600/5000 [18:40<13:19,  3.00it/s]Iter 2600: loss 2.6858\n",
            "Training Progress:  54% 2700/5000 [19:12<12:19,  3.11it/s]Iter 2700: loss 2.7145\n",
            "Training Progress:  56% 2800/5000 [19:45<11:52,  3.09it/s]Iter 2800: loss 2.7158\n",
            "Training Progress:  58% 2900/5000 [20:17<11:18,  3.10it/s]Iter 2900: loss 2.7126\n",
            "Training Progress:  60% 3000/5000 [20:49<10:42,  3.11it/s]Iter 3000: loss 2.6940\n",
            "\n",
            "Step 3000: train loss 2.7055, val loss 2.7316\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  62% 3100/5000 [22:02<10:16,  3.08it/s]Iter 3100: loss 2.7569\n",
            "Training Progress:  64% 3200/5000 [22:35<09:40,  3.10it/s]Iter 3200: loss 2.7240\n",
            "Training Progress:  66% 3300/5000 [23:07<09:09,  3.10it/s]Iter 3300: loss 2.6619\n",
            "Training Progress:  68% 3400/5000 [23:39<08:35,  3.10it/s]Iter 3400: loss 2.7137\n",
            "Training Progress:  70% 3500/5000 [24:11<08:04,  3.10it/s]Iter 3500: loss 2.6961\n",
            "\n",
            "Step 3500: train loss 2.7060, val loss 2.7316\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  72% 3600/5000 [25:30<07:41,  3.04it/s]Iter 3600: loss 2.6929\n",
            "Training Progress:  74% 3700/5000 [26:03<06:55,  3.13it/s]Iter 3700: loss 2.6977\n",
            "Training Progress:  76% 3800/5000 [26:35<06:30,  3.08it/s]Iter 3800: loss 2.7179\n",
            "Training Progress:  78% 3900/5000 [27:07<05:55,  3.09it/s]Iter 3900: loss 2.7381\n",
            "Training Progress:  80% 4000/5000 [27:40<05:22,  3.10it/s]Iter 4000: loss 2.7021\n",
            "\n",
            "Step 4000: train loss 2.6983, val loss 2.7317\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  82% 4100/5000 [28:59<05:00,  2.99it/s]Iter 4100: loss 2.7175\n",
            "Training Progress:  84% 4200/5000 [29:31<04:16,  3.12it/s]Iter 4200: loss 2.6956\n",
            "Training Progress:  86% 4300/5000 [30:03<03:47,  3.08it/s]Iter 4300: loss 2.6672\n",
            "Training Progress:  88% 4400/5000 [30:36<03:14,  3.09it/s]Iter 4400: loss 2.7059\n",
            "Training Progress:  90% 4500/5000 [31:08<02:42,  3.07it/s]Iter 4500: loss 2.6752\n",
            "\n",
            "Step 4500: train loss 2.7006, val loss 2.7293\n",
            "Saved checkpoint to out-enwik8-char/ckpt.pt\n",
            "Training Progress:  92% 4600/5000 [32:28<02:13,  3.00it/s]Iter 4600: loss 2.7360\n",
            "Training Progress:  94% 4700/5000 [33:00<01:36,  3.11it/s]Iter 4700: loss 2.6850\n",
            "Training Progress:  96% 4800/5000 [33:32<01:04,  3.09it/s]Iter 4800: loss 2.6879\n",
            "Training Progress:  98% 4900/5000 [34:05<00:32,  3.09it/s]Iter 4900: loss 2.6780\n",
            "Training Progress: 100% 5000/5000 [34:37<00:00,  3.08it/s]Iter 5000: loss 2.6803\n",
            "Training Progress: 100% 5000/5000 [34:37<00:00,  2.41it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train the baseline model\n",
        "!python train.py --config config/enwik8_char_baseline.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79RnjAeFQob1",
        "outputId": "8934c644-98a4-412b-8526-57aa4d40bfc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: out-enwik8-char-rope\n",
            "Tokens per iteration will be: 16,384\n",
            "number of parameters: 27.99M\n",
            "Using GPTWithRoPE model.\n",
            "/content/nanoGPT/train.py:111: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(config['dtype'] == 'float16'))\n",
            "Number of parameters: 30.80M\n",
            "Training Progress:   0% 0/5000 [00:00<?, ?it/s]\n",
            "Step 0: train loss 8.7755, val loss 8.7751\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:   2% 100/5000 [01:42<32:04,  2.55it/s]Iter 100: loss 2.2703\n",
            "Training Progress:   4% 200/5000 [02:21<30:51,  2.59it/s]Iter 200: loss 1.9288\n",
            "Training Progress:   6% 300/5000 [03:00<30:29,  2.57it/s]Iter 300: loss 1.8749\n",
            "Training Progress:   8% 400/5000 [03:39<29:50,  2.57it/s]Iter 400: loss 1.6522\n",
            "Training Progress:  10% 500/5000 [04:18<29:10,  2.57it/s]Iter 500: loss 1.6242\n",
            "\n",
            "Step 500: train loss 1.5567, val loss 1.5576\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  12% 600/5000 [06:04<29:11,  2.51it/s]Iter 600: loss 1.5002\n",
            "Training Progress:  14% 700/5000 [06:43<27:39,  2.59it/s]Iter 700: loss 1.4866\n",
            "Training Progress:  16% 800/5000 [07:22<27:19,  2.56it/s]Iter 800: loss 1.4766\n",
            "Training Progress:  18% 900/5000 [08:00<26:27,  2.58it/s]Iter 900: loss 1.3526\n",
            "Training Progress:  20% 1000/5000 [08:39<25:46,  2.59it/s]Iter 1000: loss 1.3606\n",
            "\n",
            "Step 1000: train loss 1.3452, val loss 1.3398\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  22% 1100/5000 [10:21<25:31,  2.55it/s]Iter 1100: loss 1.3933\n",
            "Training Progress:  24% 1200/5000 [10:59<24:23,  2.60it/s]Iter 1200: loss 1.3100\n",
            "Training Progress:  26% 1300/5000 [11:38<24:02,  2.57it/s]Iter 1300: loss 1.3102\n",
            "Training Progress:  28% 1400/5000 [12:17<23:17,  2.58it/s]Iter 1400: loss 1.2750\n",
            "Training Progress:  30% 1500/5000 [12:56<22:36,  2.58it/s]Iter 1500: loss 1.3109\n",
            "\n",
            "Step 1500: train loss 1.2536, val loss 1.2410\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  32% 1600/5000 [14:37<22:07,  2.56it/s]Iter 1600: loss 1.3185\n",
            "Training Progress:  34% 1700/5000 [15:15<21:08,  2.60it/s]Iter 1700: loss 1.2527\n",
            "Training Progress:  36% 1800/5000 [15:54<20:45,  2.57it/s]Iter 1800: loss 1.1962\n",
            "Training Progress:  38% 1900/5000 [16:33<20:07,  2.57it/s]Iter 1900: loss 1.2648\n",
            "Training Progress:  40% 2000/5000 [17:12<19:19,  2.59it/s]Iter 2000: loss 1.2743\n",
            "\n",
            "Step 2000: train loss 1.2060, val loss 1.1960\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  42% 2100/5000 [18:53<18:54,  2.56it/s]Iter 2100: loss 1.2415\n",
            "Training Progress:  44% 2200/5000 [19:31<18:03,  2.59it/s]Iter 2200: loss 1.1951\n",
            "Training Progress:  46% 2300/5000 [20:10<17:30,  2.57it/s]Iter 2300: loss 1.2682\n",
            "Training Progress:  48% 2400/5000 [20:49<16:48,  2.58it/s]Iter 2400: loss 1.1709\n",
            "Training Progress:  50% 2500/5000 [21:28<16:11,  2.57it/s]Iter 2500: loss 1.2021\n",
            "\n",
            "Step 2500: train loss 1.1630, val loss 1.1614\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  52% 2600/5000 [23:09<15:36,  2.56it/s]Iter 2600: loss 1.2420\n",
            "Training Progress:  54% 2700/5000 [23:47<14:49,  2.59it/s]Iter 2700: loss 1.1964\n",
            "Training Progress:  56% 2800/5000 [24:26<14:18,  2.56it/s]Iter 2800: loss 1.1683\n",
            "Training Progress:  58% 2900/5000 [25:05<13:34,  2.58it/s]Iter 2900: loss 1.1838\n",
            "Training Progress:  60% 3000/5000 [25:44<12:54,  2.58it/s]Iter 3000: loss 1.1551\n",
            "\n",
            "Step 3000: train loss 1.1314, val loss 1.1292\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  62% 3100/5000 [27:25<12:25,  2.55it/s]Iter 3100: loss 1.1598\n",
            "Training Progress:  64% 3200/5000 [28:04<11:33,  2.59it/s]Iter 3200: loss 1.0852\n",
            "Training Progress:  66% 3300/5000 [28:43<11:01,  2.57it/s]Iter 3300: loss 1.2039\n",
            "Training Progress:  68% 3400/5000 [29:21<10:20,  2.58it/s]Iter 3400: loss 1.1839\n",
            "Training Progress:  70% 3500/5000 [30:00<09:39,  2.59it/s]Iter 3500: loss 1.1326\n",
            "\n",
            "Step 3500: train loss 1.1101, val loss 1.1137\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  72% 3600/5000 [31:41<09:09,  2.55it/s]Iter 3600: loss 1.0676\n",
            "Training Progress:  74% 3700/5000 [32:20<08:21,  2.59it/s]Iter 3700: loss 1.2040\n",
            "Training Progress:  76% 3800/5000 [32:59<07:46,  2.57it/s]Iter 3800: loss 1.1984\n",
            "Training Progress:  78% 3900/5000 [33:37<07:06,  2.58it/s]Iter 3900: loss 1.0800\n",
            "Training Progress:  80% 4000/5000 [34:16<06:27,  2.58it/s]Iter 4000: loss 1.1455\n",
            "\n",
            "Step 4000: train loss 1.0813, val loss 1.0928\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  82% 4100/5000 [35:57<05:51,  2.56it/s]Iter 4100: loss 1.1341\n",
            "Training Progress:  84% 4200/5000 [36:36<05:08,  2.59it/s]Iter 4200: loss 1.0970\n",
            "Training Progress:  86% 4300/5000 [37:15<04:33,  2.56it/s]Iter 4300: loss 1.1106\n",
            "Training Progress:  88% 4400/5000 [37:54<03:53,  2.57it/s]Iter 4400: loss 1.1525\n",
            "Training Progress:  90% 4500/5000 [38:32<03:14,  2.57it/s]Iter 4500: loss 1.0773\n",
            "\n",
            "Step 4500: train loss 1.0700, val loss 1.0762\n",
            "Saved checkpoint to out-enwik8-char-rope/ckpt.pt\n",
            "Training Progress:  92% 4600/5000 [40:14<02:36,  2.56it/s]Iter 4600: loss 1.0473\n",
            "Training Progress:  94% 4700/5000 [40:53<01:56,  2.58it/s]Iter 4700: loss 1.1127\n",
            "Training Progress:  96% 4800/5000 [41:32<01:18,  2.56it/s]Iter 4800: loss 1.1428\n",
            "Training Progress:  98% 4900/5000 [42:10<00:38,  2.58it/s]Iter 4900: loss 1.1725\n",
            "Training Progress: 100% 5000/5000 [42:49<00:00,  2.58it/s]Iter 5000: loss 1.0895\n",
            "Training Progress: 100% 5000/5000 [42:49<00:00,  1.95it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train the modified model with RoPE\n",
        "\n",
        "!python train.py --config config/enwik8_char_rope.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGCTKJTXQpS8",
        "outputId": "586004a8-2286-4b32-bedc-d1c4722f0525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing evaluate.py\n"
          ]
        }
      ],
      "source": [
        "# Create the evaluation script\n",
        "\n",
        "%%writefile evaluate.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import math\n",
        "from model import GPTConfig, GPT\n",
        "from model_rope import GPTWithRoPE\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            with torch.amp.autocast(device_type=device):\n",
        "                logits, loss = model(x, y)\n",
        "            losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model_type', type=str, choices=['gpt', 'rope'], default='gpt', help='Model type: gpt or rope')\n",
        "    parser.add_argument('--dataset', type=str, default='enwik8', help='Dataset name')\n",
        "    parser.add_argument('--checkpoint', type=str, required=True, help='Checkpoint file')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(args.checkpoint, map_location=device)\n",
        "\n",
        "    # Load the model configuration from the checkpoint\n",
        "    ckpt_config = checkpoint['config']\n",
        "\n",
        "    # Update vocab_size from the dataset's meta.pkl\n",
        "    with open(f\"data/{args.dataset}/meta.pkl\", 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    vocab_size = meta['vocab_size']\n",
        "    ckpt_config['vocab_size'] = vocab_size\n",
        "\n",
        "    # Filter ckpt_config to only include keys that GPTConfig accepts\n",
        "    valid_config_keys = ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size', 'dropout']\n",
        "    model_config_kwargs = {k: ckpt_config[k] for k in valid_config_keys if k in ckpt_config}\n",
        "\n",
        "    # Model configuration\n",
        "    model_config = GPTConfig(**model_config_kwargs)\n",
        "\n",
        "    # Instantiate the model\n",
        "    if args.model_type == 'rope' or ckpt_config.get('model_type') == 'rope':\n",
        "        model = GPTWithRoPE(model_config)\n",
        "        print(\"Using GPTWithRoPE model.\")\n",
        "    else:\n",
        "        model = GPT(model_config)\n",
        "        print(\"Using BaselineGPT model.\")\n",
        "\n",
        "    # Load the model state\n",
        "    model.load_state_dict(checkpoint['model'], strict=False)\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare data loader\n",
        "    block_size = ckpt_config['block_size']\n",
        "    batch_size = ckpt_config.get('batch_size', 64)  # Default to 64 if not specified\n",
        "\n",
        "    # Load validation data\n",
        "    val_data = np.memmap(f'data/{args.dataset}/val.bin', dtype=np.uint16, mode='r')\n",
        "    val_data = torch.from_numpy(val_data.astype(np.int64))\n",
        "\n",
        "    # Create sequences of block_size\n",
        "    num_tokens = len(val_data) - 1\n",
        "    x_tokens = val_data[:num_tokens]\n",
        "    y_tokens = val_data[1:num_tokens+1]\n",
        "\n",
        "    # Ensure that the number of tokens is a multiple of block_size\n",
        "    num_batches = num_tokens // block_size\n",
        "    x_tokens = x_tokens[:num_batches * block_size]\n",
        "    y_tokens = y_tokens[:num_batches * block_size]\n",
        "\n",
        "    # Reshape into batches\n",
        "    x_batches = x_tokens.view(-1, block_size)\n",
        "    y_batches = y_tokens.view(-1, block_size)\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(x_batches, y_batches)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    bpc = val_loss / math.log(2)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Bits per character (bpc): {bpc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp80_W4eR5Zl",
        "outputId": "0210fc9c-0064-452d-c9de-1411e52e0b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nanoGPT/evaluate.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.checkpoint, map_location=device)\n",
            "number of parameters: 23.35M\n",
            "Using BaselineGPT model.\n",
            "Validation Loss: 4.3922, Bits per character (bpc): 6.3367\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the baseline model\n",
        "\n",
        "!python evaluate.py --model_type gpt --checkpoint out-enwik8-char/ckpt.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibkQ6GqNQwmT",
        "outputId": "a27c296f-2dd6-41c0-ea51-86760b7c8aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nanoGPT/evaluate.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.checkpoint, map_location=device)\n",
            "number of parameters: 27.99M\n",
            "Using GPTWithRoPE model.\n",
            "Validation Loss: 1.0763, Bits per character (bpc): 1.5528\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the modified model\n",
        "\n",
        "!python evaluate.py --model_type rope --checkpoint out-enwik8-char-rope/ckpt.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMoCk-WK2QBv",
        "outputId": "01e01c48-95b2-4794-b58e-544d2e8e2aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Model       |   Parameters (M) |    bpc |\n",
            "|:------------|-----------------:|-------:|\n",
            "| BaselineGPT |            23.35 | 6.3367 |\n",
            "| GPTWithRoPE |            27.99 | 1.5528 |\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Data for the table\n",
        "data = [\n",
        "    [\"BaselineGPT\", \"23.35\", \"6.3367\"],\n",
        "    [\"GPTWithRoPE\", \"27.99\", \"1.5528\"],\n",
        "]\n",
        "\n",
        "# Create the table\n",
        "headers = [\"Model\", \"Parameters (M)\", \"bpc\"]\n",
        "table = tabulate(data, headers, tablefmt=\"pipe\")\n",
        "\n",
        "# Print the table\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huKkWngd2S-h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00704724d5ff48898a7452ad4e54eb55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d5ca73b84a54b6fa7e7878374871fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118b95bec92c4c1f9c14def5c8f60d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57544fb85c434a80b9826bd0bc1836f8",
              "IPY_MODEL_c0c4824ee555440894ced5372b729712",
              "IPY_MODEL_570f62562a35410ba0e7208f6e6e1ff0"
            ],
            "layout": "IPY_MODEL_00704724d5ff48898a7452ad4e54eb55"
          }
        },
        "140d48fd8b5c49939a395b47034ab2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c4815ba89064fddbe809d4a108eae39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93d88e7e93c04af5b55990c7dc2b9d32",
            "max": 36445475,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc9df5b5a1cd4a208c7ca08005780269",
            "value": 36445475
          }
        },
        "38324ba39cc04486aed54fae9143af15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77e4540a661442248cdd4067fecf4eca",
              "IPY_MODEL_f11665315e80406e83b03afb08c69ccd",
              "IPY_MODEL_591db38f2adc44a9821e231761c66ae5"
            ],
            "layout": "IPY_MODEL_8286acc6f17343f19f494768e69b4977"
          }
        },
        "3e2f2c46720141c5bc55f5a800661767": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45df0e8172b840f2ba63338246d17464": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eac29f722ce447fbd831b17b5ceff16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff687de55884d25aecebfca802e3112",
            "placeholder": "",
            "style": "IPY_MODEL_dce6fe3abe5f4b789d7bd2e65b778f11",
            "value": "README.md:100%"
          }
        },
        "52844cdd0e184b349abcc529655f3af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_880ed764e5d94768a1f99506b1bbd4b7",
            "placeholder": "",
            "style": "IPY_MODEL_a930af4ad92940bf87877f789362fc3f",
            "value": "36.4M/36.4M[00:01&lt;00:00,37.8MB/s]"
          }
        },
        "53ff6026f9814daf8d24aae2c5e879ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "552d0d9929df484cbffb2f3e5c5269ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60e81221f54d4b46bf51f7db3a2ccffc",
            "placeholder": "",
            "style": "IPY_MODEL_53ff6026f9814daf8d24aae2c5e879ae",
            "value": "4.28k/4.28k[00:00&lt;00:00,68.0kB/s]"
          }
        },
        "570f62562a35410ba0e7208f6e6e1ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d5ca73b84a54b6fa7e7878374871fbd",
            "placeholder": "",
            "style": "IPY_MODEL_f07f0feac44d4e17bf7a1c555338e280",
            "value": "1128024/1128024[00:19&lt;00:00,21699.16examples/s]"
          }
        },
        "57544fb85c434a80b9826bd0bc1836f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c30a1b894134bebb16689211b7f6c43",
            "placeholder": "",
            "style": "IPY_MODEL_140d48fd8b5c49939a395b47034ab2e9",
            "value": "Generatingtrainsplit:100%"
          }
        },
        "591db38f2adc44a9821e231761c66ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c1c3067d3a240f3bec061d7dfa24201",
            "placeholder": "",
            "style": "IPY_MODEL_45df0e8172b840f2ba63338246d17464",
            "value": "2.94k/2.94k[00:00&lt;00:00,44.2kB/s]"
          }
        },
        "5c1c3067d3a240f3bec061d7dfa24201": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e81221f54d4b46bf51f7db3a2ccffc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f1bada96d4d41debc8f9970b8557e77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a970bc4d0849b09365530530e00a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77e4540a661442248cdd4067fecf4eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9281f2f5f2e84d688ec511bb6d05e0b3",
            "placeholder": "",
            "style": "IPY_MODEL_f94bc6c7260c448caceb6f5921e518a5",
            "value": "enwik8.py:100%"
          }
        },
        "7c30a1b894134bebb16689211b7f6c43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8286acc6f17343f19f494768e69b4977": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86d6c0a80f8548e3a93bd96ea04477ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e687cf062bd542098b32f821d49d4fc8",
            "placeholder": "",
            "style": "IPY_MODEL_bcd8de3f4b164025bed6cbafc159b0e2",
            "value": "Downloadingdata:100%"
          }
        },
        "87716ff4d7e44c799e93b6571a61cd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "880ed764e5d94768a1f99506b1bbd4b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9281f2f5f2e84d688ec511bb6d05e0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d88e7e93c04af5b55990c7dc2b9d32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3e50a7c49f44f7ab78a8dd53b4dce13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a930af4ad92940bf87877f789362fc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc9df5b5a1cd4a208c7ca08005780269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcd8de3f4b164025bed6cbafc159b0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0c4824ee555440894ced5372b729712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f1bada96d4d41debc8f9970b8557e77",
            "max": 1128024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8280cddc3f042298f8d860518fce268",
            "value": 1128024
          }
        },
        "cb2a935ac9fb495f9b74c3e212b25078": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3e50a7c49f44f7ab78a8dd53b4dce13",
            "max": 4275,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87716ff4d7e44c799e93b6571a61cd08",
            "value": 4275
          }
        },
        "d1b47e9d43e44e899757587476831fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4eac29f722ce447fbd831b17b5ceff16",
              "IPY_MODEL_cb2a935ac9fb495f9b74c3e212b25078",
              "IPY_MODEL_552d0d9929df484cbffb2f3e5c5269ef"
            ],
            "layout": "IPY_MODEL_3e2f2c46720141c5bc55f5a800661767"
          }
        },
        "d8280cddc3f042298f8d860518fce268": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcba5356ffcf4d3aaad345426363d918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dce6fe3abe5f4b789d7bd2e65b778f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e687cf062bd542098b32f821d49d4fc8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed1f2ea6ab70438e959f01067593456b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86d6c0a80f8548e3a93bd96ea04477ed",
              "IPY_MODEL_2c4815ba89064fddbe809d4a108eae39",
              "IPY_MODEL_52844cdd0e184b349abcc529655f3af7"
            ],
            "layout": "IPY_MODEL_76a970bc4d0849b09365530530e00a5a"
          }
        },
        "ef5bc403174543aa86db68ec5624e80d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07f0feac44d4e17bf7a1c555338e280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f11665315e80406e83b03afb08c69ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef5bc403174543aa86db68ec5624e80d",
            "max": 2941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcba5356ffcf4d3aaad345426363d918",
            "value": 2941
          }
        },
        "f94bc6c7260c448caceb6f5921e518a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fff687de55884d25aecebfca802e3112": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
